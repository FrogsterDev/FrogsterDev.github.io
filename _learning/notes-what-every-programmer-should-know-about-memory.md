---
layout: learning
title: "Notes from \"What Every Programmer Should Know About Memory\""
status: "Finished"
description: "Here I put all the notes I made and that helped me with understanding all of the topics... I used ChatGPT for learning purposes, that's the only reason you should ever use it TBH."

---

# ğŸ§  CPU Cache Hierarchy & Memory Latency

*ğŸ“˜ Overview* 

Modern CPUs use multiple levels of cache â€” small, ultra-fast memory units â€” to bridge the huge speed gap between the CPU and main memory (RAM).

> âš™ï¸ Problem: Each cell in the large main memory could be accessed by the CPU,
> but the cache is tiny â€” so cache implementers must decide which memory parts to store for best performance.

*ğŸ§© Cache Levels Explained*

| Level   | Name                      | Typical Size  | Speed          | Shared Between Cores | Purpose                                 |
| ------- | ------------------------- | ------------- | -------------- | -------------------- | --------------------------------------- |
| **L1d** | Level 1 Data Cache        | 32 KB         | Fastest        | No                   | Stores **data** used by the CPU         |
| **L1i** | Level 1 Instruction Cache | 32 KB         | Fastest        | No                   | Stores **instructions (code)**          |
| **L2**  | Level 2 Cache             | 256 KB â€“ 1 MB | Slower than L1 | Usually No           | Backup for L1; holds both data and code |
| **L3**  | Level 3 Cache             | 4â€“64 MB       | Slowest cache  | **Yes**              | Shared by all cores; reduces RAM access |

*âš¡ Memory Access Speed Comparison*

| Memory Type           | Approx. Access Time | Relative Speed (vs. CPU) | Typical Size   | Notes                                 |
| --------------------- | ------------------- | ------------------------ | -------------- | ------------------------------------- |
| **CPU Registers**     | ~0.3 ns             | ğŸ’¨ 1Ã— (fastest)          | Few KB         | Used directly for active computations |
| **L1 Cache**          | ~1 ns               | âš¡ ~3Ã— slower             | 32â€“64 KB       | Closest cache to the CPU              |
| **L2 Cache**          | ~3â€“5 ns             | âš™ï¸ ~10Ã— slower           | 256 KB â€“ 1 MB  | Per-core cache                        |
| **L3 Cache**          | ~10â€“20 ns           | ğŸŒ€ ~30â€“60Ã— slower        | 4â€“64 MB        | Shared across cores                   |
| **Main Memory (RAM)** | ~60â€“100 ns          | ğŸ¢ ~200â€“300Ã— slower      | GBs            | Much slower than caches               |
| **SSD Storage**       | ~100 Âµs             | ğŸª¨ ~300,000Ã— slower      | Hundreds of GB | Used for files and paging             |
| **HDD Storage**       | ~5 ms               | ğŸŒ ~10,000,000Ã— slower   | TBs            | Mechanical, extremely slow            |

*ğŸ•’ Memory Latency Timeline*
```
Registers     : |â–ˆ| 0.3 ns
L1 Cache      : |â–ˆâ”€â”€â”€â”€| 1 ns
L2 Cache      : |â–ˆâ”€â”€â”€â”€â”€â”€â”€â”€â”€| 4 ns
L3 Cache      : |â–ˆâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€| 15 ns
Main Memory   : |â–ˆâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€| 80 ns
SSD (NVMe)    : |â–ˆâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€| 100 Âµs (100,000 ns)
HDD (Disk)    : |â–ˆâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€| 5 ms (5,000,000 ns)
```

ğŸ§© Each bar represents access delay â€” the farther the memory is from the CPU, the longer it takes to reach it.

# ğŸ§  Cache Size Formula & Concepts

*Memory Address Breakdown*

> Each memory address is split into three parts: `[ TAG ğŸ·ï¸ | Set Index ğŸ—‚ï¸ | Block Offset ğŸ“¦ ]`
- TAG ğŸ·ï¸ â†’ Identifies which memory block is stored in a line.
- Set Index ğŸ—‚ï¸ â†’ Determines which set the block maps to.
- Block Offset ğŸ“¦ â†’ Identifies the byte within the cache line.

*Cache Structure*


    CACHE ğŸ’¾ (Total Size)
    +---------------------------------------------+
    | Set 0 ğŸ—‚ï¸                                    |
    |  +------------+  +------------+ ...         |
    |  | Line 0 ğŸ“¦  |  | Line 1 ğŸ“¦  | ...         |
    |  | TAG ğŸ·ï¸     |  | TAG ğŸ·ï¸     |             |
    |  +------------+  +------------+             |
    | Set 1 ğŸ—‚ï¸                                    |
    |  +------------+  +------------+ ...         |
    |  | Line 0 ğŸ“¦  |  | Line 1 ğŸ“¦  | ...         |
    |  | TAG ğŸ·ï¸     |  | TAG ğŸ·ï¸     |             |
    |  +------------+  +------------+             |
    | Set 2 ğŸ—‚ï¸                                    |
    |  +------------+  +------------+ ...         |
    |  | Line 0 ğŸ“¦  |  | Line 1 ğŸ“¦  | ...         |
    |  | TAG ğŸ·ï¸     |  | TAG ğŸ·ï¸     |             |
    |  +------------+  +------------+             |
    | ...                                         |
    | Set N ğŸ—‚ï¸                                    |
    +---------------------------------------------+


Formula:

`CacheÂ size` = `CacheÂ lineÂ size` Ã— `Associativity` Ã— `NumberÂ ofÂ sets`

1ï¸âƒ£ *Cache Line Size* ğŸ“¦

- Smallest unit of memory transfer between RAM â†” CPU cache.
- Typical: 64 bytes.
- Exploits spatial locality: accessing nearby memory is faster.
- Analogy: like buying a pack of apples ğŸ; even if you need 1, you get the whole pack.

2ï¸âƒ£ *Associativity* ğŸ”—

- Number of cache lines per set.
- Determines how many blocks can fit in the same set.

> *Types:*
>
| Type              | Associativity | Description                                                       |
| ----------------- | ------------- | ----------------------------------------------------------------- |
| Direct-mapped     | 1             | Each address maps to **one line** only.                           |
| Set-associative   | 2,4,8â€¦        | Each set can hold multiple lines â†’ reduces **conflict misses âš¡**. |
| Fully associative | # of lines    | Any memory block can go anywhere in cache ğŸŒ.                     |


3ï¸âƒ£ *Number of Sets* ğŸ—‚ï¸

- Number of distinct sets in the cache.

> - *Relationship:* `NumberÂ ofÂ sets` =  `TotalÂ cacheÂ lines`â€‹ / `Associativity`

- Memory addresses use set bits to determine which set they map to.

ğŸ’¡ *Example*

- Cache line size = 64 bytes ğŸ“¦
- 8-way associative ğŸ”—
- 512 sets ğŸ—‚ï¸

# ğŸ§© MESI Cache Coherence Protocol

MESI ensures that multiple CPU caches stay consistent when reading/writing the same memory.

1ï¸âƒ£ *States of a Cache Line*

- `I` âŒ  Invalid     â†’ Data is not in cache or is stale
- `S` ğŸ¤  Shared      â†’ Data is in one or more caches, matches memory
- `E` ğŸ   Exclusive   â†’ Only this cache has it, matches memory
- `M` ğŸ–Šï¸  Modified    â†’ Only this cache has it, differs from memory

2ï¸âƒ£ *State Transitions (Simplified)*

*Read Miss*
>```
I âŒ (no data)  â†’ fetch from memory
        â†˜
         E ğŸ   if no other cache has it
         S ğŸ¤  if other caches have it
```

*Write Hit*
>```
S ğŸ¤ (shared)  â†’ invalidate others â†’ M ğŸ–Šï¸
E ğŸ  (exclusive) â†’ modify locally â†’ M ğŸ–Šï¸
M ğŸ–Šï¸ â†’ modify locally â†’ stay M ğŸ–Šï¸
```

*Write Miss*
>```
I âŒ â†’ fetch line â†’ invalidate other caches â†’ M ğŸ–Šï¸
```

3ï¸âƒ£ *Visual Cheat Sheet*

            +---------------------+
            |  CPU wants to read  |
            +---------------------+
                        |
                        v
        I âŒ â†’ fetch line â†’ E ğŸ  or S ğŸ¤
                        |
                        v
                CPU wants to write
                        |
     +------------------+------------------+
     |                                     |
     v                                     v
    S ğŸ¤ â†’ invalidate others â†’ M ğŸ–Šï¸      E ğŸ  â†’ modify locally â†’ M ğŸ–Šï¸
    M ğŸ–Šï¸ â†’ modify locally â†’ stay M ğŸ–Šï¸

4ï¸âƒ£ *Example Scenario*

- Core 1 has a line in S ğŸ¤
- Core 2 wants to write to it
- Core 1â€™s line becomes I âŒ
- Core 2â€™s line becomes M ğŸ–Šï¸
- Memory is updated only when M ğŸ–Šï¸ is replaced or written back

âœ… This is exactly how MESI prevents stale reads/writes in multi-core CPUs.


# ğŸ’¾ Virtual Memory, Physical Memory & TLB

1ï¸âƒ£ *Physical Memory (RAM)* ğŸ§±

- Real hardware memory (RAM) used by the CPU.
- Each address here is a physical address.
- Accessing it directly would cause:
    - Fragmentation
    - Security issues
    - Process interference
- Thatâ€™s why CPUs use virtual memory on top of it.

2ï¸âƒ£ *Virtual Memory (VMEM)* ğŸ§ 

- Provides each process with its own address space (isolated).
- Virtual addresses are mapped to physical addresses by the MMU (Memory Management Unit).
- Enables:
    - Isolation ğŸ›¡ï¸: one process canâ€™t access anotherâ€™s memory.
    - Paging ğŸ“„: divide memory into small equal-sized blocks.
    - Swapping ğŸ”„: move inactive pages to disk when RAM is full.

3ï¸âƒ£ *The MMU (Memory Management Unit)* âš™ï¸

- Hardware that performs address translation: `Virtual Address` â†’ `Physical Address`.
- Uses the page table to find which frame corresponds to which page.
- This translation happens for every memory access, so speed matters âš¡.

4ï¸âƒ£ *The TLB (Translation Lookaside Buffer)* âš¡

TLB = tiny cache inside the CPU for address translations.

| Concept      | Description                                         | Emoji |
| ------------ | --------------------------------------------------- | ----- |
| **TLB**      | Stores recently used **page â†’ frame mappings**      | âš¡    |
| **TLB Hit**  | The mapping is found in the TLB â†’ fast access ğŸš€    | âœ…    |
| **TLB Miss** | Mapping not in TLB â†’ must read page table (slow) ğŸ¢ | âŒ    |

TLB Process Overview
>```
CPU â†’ Virtual Address ğŸ§ 
       |
       v
Check TLB âš¡
   |     \
 Hit âœ…   Miss âŒ
   |        |
Use frame   â†’ Consult Page Table ğŸ§¾
             â†’ Update TLB âš¡
             â†’ Access memory ğŸ§±
```

**TLB + Virtual Memory = Fast & Safe Memory Access**
| Step | Component                   | Action |
| ---- | --------------------------- | ------ |
| 1    | CPU issues virtual address  | ğŸ§      |
| 2    | TLB translates (if hit)     | âš¡     |
| 3    | If miss â†’ Page table lookup | ğŸ§¾     |
| 4    | MMU uses physical address   | ğŸ§±     |
| 5    | Cache/memory access happens | ğŸš€     |

5ï¸âƒ£ *Putting It All Together*

```
Virtual Memory (per process) ğŸ§ 
       â†“  (translated by)
      MMU âš™ï¸  â†’  uses TLB âš¡ + Page Tables ğŸ§¾
       â†“
Physical Memory (RAM) ğŸ§±
```

---

# ğŸ”¢ Matrix multiplication optimization

A straightforward matrix multiplication like:
```cpp
for (int i = 0; i < N; i++)
  for (int j = 0; j < N; j++)
    for (int k = 0; k < N; k++)
      res[i][j] += mul1[i][k] * mul2[k][j];
```
looks fine, but it performs **very poorly on modern CPUs** when `N` is large.

The issue? **Cache performance.**
>
- CPUs fetch memory in cache lines (typically 64 bytes).
- When mul2[k][j] is accessed, weâ€™re jumping across columns of mul2, meaning weâ€™re skipping large strides in memory.
- That causes cache misses, because each access likely requires loading a new cache line from main memory.
- So, most CPU cycles are wasted waiting for memory loads instead of doing multiplications.

âš™ï¸ *What the optimized version does*

```cpp
#define SM (CLS / sizeof(double))  // CLS = cache line size, so SM = number of doubles per cache line

for (i = 0; i < N; i += SM)
for (j = 0; j < N; j += SM)
for (k = 0; k < N; k += SM)
  for (i2 = 0, rres = &res[i][j],
              rmul1 = &mul1[i][k];
       i2 < SM;
       ++i2, rres += N, rmul1 += N)
    for (k2 = 0, rmul2 = &mul2[k][j];
         k2 < SM;
         ++k2, rmul2 += N)
      for (j2 = 0; j2 < SM; ++j2)
        rres[j2] += rmul1[k2] * rmul2[j2];
```

ğŸ§  *What this actually means*

This algorithm divides the matrix into small sub-blocks (tiles) of size `SM Ã— SM`, where `SM` is chosen so that each block fits in the CPU cache.

Each of these outer loops (`i`, `j`, `k`) processes one block at a time.

Inside those blocks, we do the actual multiplication, but now:
>
- We reuse the same small chunks of data (mul1, mul2, and res) multiple times, 
- while they are still hot in the CPU cache.

ğŸ” *Why this is faster*

1. Cache locality
    - When you operate on small `SMÃ—SM` blocks, all the needed values stay in cache.
    - This avoids repeatedly loading the same data from main memory, drastically reducing cache misses.

2. Spatial locality
    - Accesses like rres[j2], rmul1[k2], and rmul2[j2] are contiguous or near-contiguous in memory.
    - The CPU prefetcher can easily predict and fetch data in advance.

3. Temporal locality
    - Each value in mul1 and mul2 is reused multiple times before being evicted from the cache.

4. Better hardware utilization
    - Modern CPUs have vector units (SIMD), large caches, and memory prefetchers optimized for sequential access.
    - The blocked structure aligns with how CPUs like to operate: working on small contiguous data chunks.

5. Reduced TLB (Translation Lookaside Buffer) misses
    - Working within small memory regions means fewer page lookups, which reduces virtual memory overhead.

---