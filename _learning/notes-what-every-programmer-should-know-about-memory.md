---
layout: learning
title: "Notes from \"What Every Programmer Should Know About Memory\""
status: "Finished"
description: "Here I put all the notes I made and that helped me with understanding all of the topics... I used ChatGPT for learning purposes, that's the only reason you should ever use it TBH."

---

# ğŸ§  CPU Cache Hierarchy & Memory Latency

*ğŸ“˜ Overview* 

Modern CPUs use multiple levels of cache â€” small, ultra-fast memory units â€” to bridge the huge speed gap between the CPU and main memory (RAM).

> âš™ï¸ Problem: Each cell in the large main memory could be accessed by the CPU,
> but the cache is tiny â€” so cache implementers must decide which memory parts to store for best performance.

*ğŸ§© Cache Levels Explained*

| Level   | Name                      | Typical Size  | Speed          | Shared Between Cores | Purpose                                 |
| ------- | ------------------------- | ------------- | -------------- | -------------------- | --------------------------------------- |
| **L1d** | Level 1 Data Cache        | 32 KB         | Fastest        | No                   | Stores **data** used by the CPU         |
| **L1i** | Level 1 Instruction Cache | 32 KB         | Fastest        | No                   | Stores **instructions (code)**          |
| **L2**  | Level 2 Cache             | 256 KB â€“ 1 MB | Slower than L1 | Usually No           | Backup for L1; holds both data and code |
| **L3**  | Level 3 Cache             | 4â€“64 MB       | Slowest cache  | **Yes**              | Shared by all cores; reduces RAM access |

*âš¡ Memory Access Speed Comparison*

| Memory Type           | Approx. Access Time | Relative Speed (vs. CPU) | Typical Size   | Notes                                 |
| --------------------- | ------------------- | ------------------------ | -------------- | ------------------------------------- |
| **CPU Registers**     | ~0.3 ns             | ğŸ’¨ 1Ã— (fastest)          | Few KB         | Used directly for active computations |
| **L1 Cache**          | ~1 ns               | âš¡ ~3Ã— slower             | 32â€“64 KB       | Closest cache to the CPU              |
| **L2 Cache**          | ~3â€“5 ns             | âš™ï¸ ~10Ã— slower           | 256 KB â€“ 1 MB  | Per-core cache                        |
| **L3 Cache**          | ~10â€“20 ns           | ğŸŒ€ ~30â€“60Ã— slower        | 4â€“64 MB        | Shared across cores                   |
| **Main Memory (RAM)** | ~60â€“100 ns          | ğŸ¢ ~200â€“300Ã— slower      | GBs            | Much slower than caches               |
| **SSD Storage**       | ~100 Âµs             | ğŸª¨ ~300,000Ã— slower      | Hundreds of GB | Used for files and paging             |
| **HDD Storage**       | ~5 ms               | ğŸŒ ~10,000,000Ã— slower   | TBs            | Mechanical, extremely slow            |

*ğŸ•’ Memory Latency Timeline*
```
Registers     : |â–ˆ| 0.3 ns
L1 Cache      : |â–ˆâ”€â”€â”€â”€| 1 ns
L2 Cache      : |â–ˆâ”€â”€â”€â”€â”€â”€â”€â”€â”€| 4 ns
L3 Cache      : |â–ˆâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€| 15 ns
Main Memory   : |â–ˆâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€| 80 ns
SSD (NVMe)    : |â–ˆâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€| 100 Âµs (100,000 ns)
HDD (Disk)    : |â–ˆâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€| 5 ms (5,000,000 ns)
```

ğŸ§© Each bar represents access delay â€” the farther the memory is from the CPU, the longer it takes to reach it.

# ğŸ§  Cache Size Formula & Concepts

*Memory Address Breakdown*

> Each memory address is split into three parts: `[ TAG ğŸ·ï¸ | Set Index ğŸ—‚ï¸ | Block Offset ğŸ“¦ ]`
- TAG ğŸ·ï¸ â†’ Identifies which memory block is stored in a line.
- Set Index ğŸ—‚ï¸ â†’ Determines which set the block maps to.
- Block Offset ğŸ“¦ â†’ Identifies the byte within the cache line.

*Cache Structure*


    CACHE ğŸ’¾ (Total Size)
    +---------------------------------------------+
    | Set 0 ğŸ—‚ï¸                                    |
    |  +------------+  +------------+ ...         |
    |  | Line 0 ğŸ“¦  |  | Line 1 ğŸ“¦  | ...         |
    |  | TAG ğŸ·ï¸     |  | TAG ğŸ·ï¸     |             |
    |  +------------+  +------------+             |
    | Set 1 ğŸ—‚ï¸                                    |
    |  +------------+  +------------+ ...         |
    |  | Line 0 ğŸ“¦  |  | Line 1 ğŸ“¦  | ...         |
    |  | TAG ğŸ·ï¸     |  | TAG ğŸ·ï¸     |             |
    |  +------------+  +------------+             |
    | Set 2 ğŸ—‚ï¸                                    |
    |  +------------+  +------------+ ...         |
    |  | Line 0 ğŸ“¦  |  | Line 1 ğŸ“¦  | ...         |
    |  | TAG ğŸ·ï¸     |  | TAG ğŸ·ï¸     |             |
    |  +------------+  +------------+             |
    | ...                                         |
    | Set N ğŸ—‚ï¸                                    |
    +---------------------------------------------+


Formula:

`CacheÂ size` = `CacheÂ lineÂ size` Ã— `Associativity` Ã— `NumberÂ ofÂ sets`

1ï¸âƒ£ *Cache Line Size* ğŸ“¦

- Smallest unit of memory transfer between RAM â†” CPU cache.
- Typical: 64 bytes.
- Exploits spatial locality: accessing nearby memory is faster.
- Analogy: like buying a pack of apples ğŸ; even if you need 1, you get the whole pack.

2ï¸âƒ£ *Associativity* ğŸ”—

- Number of cache lines per set.
- Determines how many blocks can fit in the same set.

> *Types:*
>
| Type              | Associativity | Description                                                       |
| ----------------- | ------------- | ----------------------------------------------------------------- |
| Direct-mapped     | 1             | Each address maps to **one line** only.                           |
| Set-associative   | 2,4,8â€¦        | Each set can hold multiple lines â†’ reduces **conflict misses âš¡**. |
| Fully associative | # of lines    | Any memory block can go anywhere in cache ğŸŒ.                     |


3ï¸âƒ£ *Number of Sets* ğŸ—‚ï¸

- Number of distinct sets in the cache.

> - *Relationship:* `NumberÂ ofÂ sets` =  `TotalÂ cacheÂ lines`â€‹ / `Associativity`

- Memory addresses use set bits to determine which set they map to.

ğŸ’¡ *Example*

- Cache line size = 64 bytes ğŸ“¦
- 8-way associative ğŸ”—
- 512 sets ğŸ—‚ï¸

# ğŸ§© MESI Cache Coherence Protocol

MESI ensures that multiple CPU caches stay consistent when reading/writing the same memory.

1ï¸âƒ£ *States of a Cache Line*

- `I` âŒ  Invalid     â†’ Data is not in cache or is stale
- `S` ğŸ¤  Shared      â†’ Data is in one or more caches, matches memory
- `E` ğŸ   Exclusive   â†’ Only this cache has it, matches memory
- `M` ğŸ–Šï¸  Modified    â†’ Only this cache has it, differs from memory

2ï¸âƒ£ *State Transitions (Simplified)*

*Read Miss*
>```
I âŒ (no data)  â†’ fetch from memory
        â†˜
         E ğŸ   if no other cache has it
         S ğŸ¤  if other caches have it
```

*Write Hit*
>```
S ğŸ¤ (shared)  â†’ invalidate others â†’ M ğŸ–Šï¸
E ğŸ  (exclusive) â†’ modify locally â†’ M ğŸ–Šï¸
M ğŸ–Šï¸ â†’ modify locally â†’ stay M ğŸ–Šï¸
```

*Write Miss*
>```
I âŒ â†’ fetch line â†’ invalidate other caches â†’ M ğŸ–Šï¸
```

3ï¸âƒ£ *Visual Cheat Sheet*

            +---------------------+
            |  CPU wants to read  |
            +---------------------+
                        |
                        v
        I âŒ â†’ fetch line â†’ E ğŸ  or S ğŸ¤
                        |
                        v
                CPU wants to write
                        |
     +------------------+------------------+
     |                                     |
     v                                     v
    S ğŸ¤ â†’ invalidate others â†’ M ğŸ–Šï¸      E ğŸ  â†’ modify locally â†’ M ğŸ–Šï¸
    M ğŸ–Šï¸ â†’ modify locally â†’ stay M ğŸ–Šï¸

4ï¸âƒ£ *Example Scenario*

- Core 1 has a line in S ğŸ¤
- Core 2 wants to write to it
- Core 1â€™s line becomes I âŒ
- Core 2â€™s line becomes M ğŸ–Šï¸
- Memory is updated only when M ğŸ–Šï¸ is replaced or written back

âœ… This is exactly how MESI prevents stale reads/writes in multi-core CPUs.


# ğŸ’¾ Virtual Memory, Physical Memory & TLB

1ï¸âƒ£ *Physical Memory (RAM)* ğŸ§±

- Real hardware memory (RAM) used by the CPU.
- Each address here is a physical address.
- Accessing it directly would cause:
    - Fragmentation
    - Security issues
    - Process interference
- Thatâ€™s why CPUs use virtual memory on top of it.

2ï¸âƒ£ *Virtual Memory (VMEM)* ğŸ§ 

- Provides each process with its own address space (isolated).
- Virtual addresses are mapped to physical addresses by the MMU (Memory Management Unit).
- Enables:
    - Isolation ğŸ›¡ï¸: one process canâ€™t access anotherâ€™s memory.
    - Paging ğŸ“„: divide memory into small equal-sized blocks.
    - Swapping ğŸ”„: move inactive pages to disk when RAM is full.

3ï¸âƒ£ *The MMU (Memory Management Unit)* âš™ï¸

- Hardware that performs address translation: `Virtual Address` â†’ `Physical Address`.
- Uses the page table to find which frame corresponds to which page.
- This translation happens for every memory access, so speed matters âš¡.

4ï¸âƒ£ *The TLB (Translation Lookaside Buffer)* âš¡

TLB = tiny cache inside the CPU for address translations.

| Concept      | Description                                         | Emoji |
| ------------ | --------------------------------------------------- | ----- |
| **TLB**      | Stores recently used **page â†’ frame mappings**      | âš¡    |
| **TLB Hit**  | The mapping is found in the TLB â†’ fast access ğŸš€    | âœ…    |
| **TLB Miss** | Mapping not in TLB â†’ must read page table (slow) ğŸ¢ | âŒ    |

TLB Process Overview
>```
CPU â†’ Virtual Address ğŸ§ 
       |
       v
Check TLB âš¡
   |     \
 Hit âœ…   Miss âŒ
   |        |
Use frame   â†’ Consult Page Table ğŸ§¾
             â†’ Update TLB âš¡
             â†’ Access memory ğŸ§±
```

**TLB + Virtual Memory = Fast & Safe Memory Access**
| Step | Component                   | Action |
| ---- | --------------------------- | ------ |
| 1    | CPU issues virtual address  | ğŸ§      |
| 2    | TLB translates (if hit)     | âš¡     |
| 3    | If miss â†’ Page table lookup | ğŸ§¾     |
| 4    | MMU uses physical address   | ğŸ§±     |
| 5    | Cache/memory access happens | ğŸš€     |

5ï¸âƒ£ *Putting It All Together*

```
Virtual Memory (per process) ğŸ§ 
       â†“  (translated by)
      MMU âš™ï¸  â†’  uses TLB âš¡ + Page Tables ğŸ§¾
       â†“
Physical Memory (RAM) ğŸ§±
```

---

# ğŸ”¢ Matrix multiplication optimization

A straightforward matrix multiplication like:
```cpp
for (int i = 0; i < N; i++)
  for (int j = 0; j < N; j++)
    for (int k = 0; k < N; k++)
      res[i][j] += mul1[i][k] * mul2[k][j];
```
looks fine, but it performs **very poorly on modern CPUs** when `N` is large.

The issue? **Cache performance.**
>
- CPUs fetch memory in cache lines (typically 64 bytes).
- When mul2[k][j] is accessed, weâ€™re jumping across columns of mul2, meaning weâ€™re skipping large strides in memory.
- That causes cache misses, because each access likely requires loading a new cache line from main memory.
- So, most CPU cycles are wasted waiting for memory loads instead of doing multiplications.

âš™ï¸ *What the optimized version does*

```cpp
#define SM (CLS / sizeof(double))  // CLS = cache line size, so SM = number of doubles per cache line

for (i = 0; i < N; i += SM)
for (j = 0; j < N; j += SM)
for (k = 0; k < N; k += SM)
  for (i2 = 0, rres = &res[i][j],
              rmul1 = &mul1[i][k];
       i2 < SM;
       ++i2, rres += N, rmul1 += N)
    for (k2 = 0, rmul2 = &mul2[k][j];
         k2 < SM;
         ++k2, rmul2 += N)
      for (j2 = 0; j2 < SM; ++j2)
        rres[j2] += rmul1[k2] * rmul2[j2];
```

ğŸ§  *What this actually means*

This algorithm divides the matrix into small sub-blocks (tiles) of size `SM Ã— SM`, where `SM` is chosen so that each block fits in the CPU cache.

Each of these outer loops (`i`, `j`, `k`) processes one block at a time.

Inside those blocks, we do the actual multiplication, but now:
>
- We reuse the same small chunks of data (mul1, mul2, and res) multiple times, 
- while they are still hot in the CPU cache.

ğŸ” *Why this is faster*

1. Cache locality
    - When you operate on small `SMÃ—SM` blocks, all the needed values stay in cache.
    - This avoids repeatedly loading the same data from main memory, drastically reducing cache misses.

2. Spatial locality
    - Accesses like rres[j2], rmul1[k2], and rmul2[j2] are contiguous or near-contiguous in memory.
    - The CPU prefetcher can easily predict and fetch data in advance.

3. Temporal locality
    - Each value in mul1 and mul2 is reused multiple times before being evicted from the cache.

4. Better hardware utilization
    - Modern CPUs have vector units (SIMD), large caches, and memory prefetchers optimized for sequential access.
    - The blocked structure aligns with how CPUs like to operate: working on small contiguous data chunks.

5. Reduced TLB (Translation Lookaside Buffer) misses
    - Working within small memory regions means fewer page lookups, which reduces virtual memory overhead.

ğŸ’¡ *Why the SM value matters*

`SM` (small matrix block size) is chosen to match the cache line size or a multiple of it:
```cpp
#define SM (CLS / sizeof(double))
```

So if the cache line is 64 bytes, and `sizeof(double)` is 8 bytes â†’ `SM = 8`.

That means each block fits neatly into a few cache lines, making cache use optimal.

For larger systems, you might tune SM based on L1/L2 cache size, not just cache line size, for even better performance.

ğŸ§© *Analogy*
Imagine the naÃ¯ve approach like trying to paint a giant mural by painting one pixel in the top-left corner, then walking to the far bottom-right to paint another pixel â€” constantly walking across the room.

The blocked version, instead, paints small 8Ã—8 sections at a time, staying in one area until itâ€™s done. Much less walking, much faster progress.

âœ… *Summary*


| Property      | NaÃ¯ve Triple Loop            | Cache-Blocked Version                          |
| ------------- | ---------------------------- | ---------------------------------------------- |
| Cache use     | Poor â€” frequent cache misses | Excellent â€” tiles fit in cache                 |
| Memory access | Strided, unpredictable       | Local and sequential                           |
| Reuse of data | Low                          | High                                           |
| Performance   | O(NÂ³) but memory-bound       | Still O(NÂ³) but CPU-bound (faster in practice) |

So in short:
>
**ğŸï¸ Itâ€™s faster because it multiplies submatrices that fit in cache, dramatically improving data locality and reducing cache misses.**

---

# ğŸ§© How CPUs load structures from memory into cache lines

âš™ï¸ *Background: How the CPU Reads Memory*

When the CPU reads any variable from memory, it doesnâ€™t just fetch that one variable.
It loads a cache line (typically 64 bytes) containing that variable and nearby data.

So:
>
- Accessing nearby variables is fast (theyâ€™re already in the cache line).
- Accessing far-apart variables may be slow (cache miss â†’ needs memory fetch).

This leads to the concept of **spatial locality**: data close together is faster to access.

*Rule* 1ï¸âƒ£ â€” â€œPut the critical word firstâ€

>
**Always move the structure element thatâ€™s most likely to be the critical word to the beginning of the structure.**

ğŸ” *What it means*

The **â€œcritical wordâ€** is the field that will most likely be accessed first (or most often) in your structure.

By placing it **at the beginning**, you ensure itâ€™s **loaded into cache first**, along with the rest of the structure if needed.

ğŸ’¡ *Why it helps*

- When the CPU fetches the first field, it loads the first 64 bytes (the cache line).
- If your structureâ€™s most-used field is at the start, the CPU gets it immediately.
- The rest of the structure might also get fetched â€œfor freeâ€ (since itâ€™s in the same cache line).

If the critical word were deeper in the structure:
- The CPU might fetch unnecessary data first.
- Then it would need a second cache line for the field you actually need â†’ *slower*.

âœ… *Example*

```cpp
// âŒ Bad
struct Player {
    char name[32];
    int id;
    float health;   // frequently accessed
    float stamina;
};
```

Here, `health` is 36 bytes into the structure â€” likely in the second cache line.

```cpp
// âœ… Good
struct Player {
    float health;   // most critical â†’ placed first
    float stamina;
    int id;
    char name[32];
};
```
Now, reading `health` (the critical word) pulls the whole first part of the structure efficiently into cache.

*Rule* 2ï¸âƒ£ â€” â€œAccess fields in the order theyâ€™re definedâ€

>**When accessing data structures, and the order of access is not dictated by the situation, access the elements in the order they are defined in the structure.**

ğŸ” *What it means*

When you read or write several structure fields one after another, try to do it in the same order as they appear in memory.

ğŸ’¡ *Why it helps*
- CPU caches and hardware prefetchers work best with sequential memory access.
- Accessing fields in the defined order means youâ€™re walking forward through memory.
- Accessing them out of order can cause unnecessary cache line loads or store delays.

âœ… *Example*
```cpp
// Structure layout
struct Car {
    int id;          // offset 0
    float speed;     // offset 4
    float fuel;      // offset 8
    char model[16];  // offset 12
};

// âŒ Bad access order
car.fuel = 10.0;
car.id = 5;
car.speed = 100.0;

// âœ… Good access order (same as declaration)
car.id = 5;
car.speed = 100.0;
car.fuel = 10.0;
```

Accessing fields in memory order helps the CPUâ€™s data prefetcher and avoids reloading cache lines unnecessarily.

ğŸ§  *Summary of Both Rules*


| Rule | What to Do | Why It Helps |
| ---- | ---- | ---- |
| **1. Place critical fields first** | Put the most-used field at the beginning of the struct | Ensures the CPU fetches important data first (better cache use) |
| **2. Access fields in declaration order** | Read/write fields in the same order as defined | Improves spatial locality and allows efficient prefetching |

âš¡ *In Short*

>ğŸ§  The CPU fetches memory in `cache lines`, not single variables.
So, **ordering matters** â€” both in structure layout and in access pattern.

Follow these two rules to:
- Reduce cache misses ğŸ§±
- Improve memory locality ğŸ§ 
- Boost real-world performance ğŸš€

---

# ğŸ§© Data structure splitting (or structure splitting).

ğŸ§  *Understanding Cache-Aware Struct Design*

âœ… *Example struct*

```cpp
struct order {
    double price;         // 8 bytes
    bool paid;            // 1 byte (plus padding)
    const char *buyer[5]; // 5 pointers (typically 8 bytes each â†’ 40 bytes)
    long buyer_id;        // 8 bytes
};
```

ğŸ”¹ *Scenario*

We have a large array of these order structures:

```cpp
struct order orders[100000];
```

and we run a frequent job that does this:

```cpp
double total = 0.0;
for (int i = 0; i < N; i++)
    if (!orders[i].paid)
        total += orders[i].price;
```

So, the code only needs:
- `price`
- `paid`

It never uses:
- `buyer`
- `buyer_id`

ğŸš¨ *Problem: Unnecessary Data Loaded into Cache*
When the CPU accesses `orders[i].paid`, it loads an entire cache line (e.g., 64 bytes) from memory â€” **not just that one field**.

Because all fields (`price`, `paid`, `buyer`, `buyer_id`) are close together in memory, each cache line also contains:

- Large, **unused** parts of the structure (`buyer`, `buyer_id`).

So for every order you process:
- The CPU loads much more data than it actually needs.
- This wastes cache space and bandwidth.

ğŸ’¡ The CPUâ€™s cache fills up with unnecessary data, which **evicts useful data** from cache sooner â€” this is called **cache pollution**.

ğŸ“‰ *Performance Impact*
>â€œJudging from the data, the program will perform up to **5Ã— slower** than it could.â€

Why?
- The CPUâ€™s cache can only hold a limited number of cache lines (e.g., 32 KB L1 cache).
- If every cache line contains unused fields, fewer `price` and `paid` fields fit at once.
- The CPU must constantly reload data from **main memory**, which is hundreds of times slower.

âœ… *Solution: Structure Splitting*

Instead of keeping everything in one large struct, **split it** into two smaller structs â€” one for frequently used fields, and one for rarely used ones.

âœ‚ï¸ *Before (inefficient)*
```cpp
struct order {
    double price;
    bool paid;
    const char *buyer[5];
    long buyer_id;
};
```

âš™ï¸ *After (split into two structures)*
```cpp
struct order_summary {
    double price;
    bool paid;
};

struct order_details {
    const char *buyer[5];
    long buyer_id;
};

struct order {
    struct order_summary summary;
    struct order_details *details;
};
```

Now, the frequent job works only with `order_summary`, which is small and fits efficiently into cache.

ğŸš€ *Benefits*
>
| Benefit | Explanation |
| ------ | ----- |
| ğŸ§  **Better cache utilization** | The CPU loads only `price` and `paid`, not the unused fields. |
| ğŸš… **Fewer cache misses** | More `order_summary` entries fit in the cache at once. |
| âš¡ **Faster iteration** | The job reads sequential, compact memory â€” ideal for prefetching. |
| ğŸ’¾ **Less memory bandwidth waste** | Only needed data travels from RAM â†’ CPU. |


âš–ï¸ *Trade-offs*
>
| Advantage | Disadvantage |
| ----- | ----- |
| Major speedup for frequent â€œsummaryâ€ operations | Slightly more complex code |
| Lower memory traffic | Slightly higher pointer indirection |
| More control over cache layout | Requires maintaining two related structures |

ğŸ§­ *Key Takeaways*
>
| Concept | Explanation |
| ----- | ----- |
| **Cache line** | CPU loads memory in 64-byte chunks â€” not individual variables. |
| **Cache pollution** | Loading unnecessary fields wastes cache space. |
| **Structure splitting** | Separate frequently used fields from rarely used ones. |
| **Result** | Up to **5Ã— faster** performance when iterating large arrays. |

# ğŸ§© The order of variables in a `struct` (or `class`) in C/C++

ğŸ§  *Struct Layout & Memory Alignment in C/C++*

ğŸ”¹ *Why Variable Order Matters*

Each variable in a struct must be **aligned** in memory according to its type **alignment requirement** (typically equal to its size).

For example:
| Type      | Size                | Alignment (usually) |
| --------- | ------------------- | ------------------- |
| `char`    | 1 byte              | 1 byte              |
| `short`   | 2 bytes             | 2 bytes             |
| `int`     | 4 bytes             | 4 bytes             |
| `double`  | 8 bytes             | 8 bytes             |
| `bool`    | 1 byte              | 1 byte              |
| `pointer` | 8 bytes (on 64-bit) | 8 bytes             |

When alignment requirements arenâ€™t met, the compiler **inserts invisible padding bytes** between fields.

So, **ordering fields incorrectly** can waste memory due to this padding.

ğŸ” *Example 1 â€” Bad Layout (wastes space)*

```cpp
struct Bad {
    char a;     // 1 byte
    double b;   // 8 bytes, needs 8-byte alignment
    int c;      // 4 bytes
};
```


**Memory layout visualization**
```cpp
| a | PAD(7) | b(8 bytes) | c(4 bytes) | PAD(4) |
```

Total size = 24 bytes
>
- 7 bytes of padding before `b`
- 4 bytes of padding at the end (for 8-byte alignment of the struct)

âœ… *Example 2 â€” Good Layout (no wasted space)*

```cpp
struct Good {
    double b;   // 8 bytes
    int c;      // 4 bytes
    char a;     // 1 byte
};
```

**Memory layout visualization**
```cpp
| b(8 bytes) | c(4 bytes) | a(1 byte) | PAD(3) |
```

Total size = 16 bytes
>
- âœ… Only 3 bytes padding at the end (required to align struct itself).
- ğŸ’¡ 8 bytes smaller than `Bad`.

ğŸ§© *Rule of Thumb for Compact Structs*

**Order fields by decreasing alignment/size.**

In other words:
- Largest types first (double, long, pointers)
- Then medium (int, float)
- Then smallest (char, bool)

âœ… This minimizes internal padding.

ğŸ§® *Example 3 â€” Combining fields of same size*

```cpp
struct Compact {
    double d1, d2;
    int i1, i2;
    char c1, c2;
};
```

No padding needed inside the groups â€” only possibly at the end.

âš ï¸ *Example 4 â€” How bools can waste a lot of space*

```cpp
struct Flags {
    bool a;
    bool b;
    bool c;
    bool d;
};
```

Each `bool` is **1 byte**, but alignment might add padding between them on some compilers, or make the struct **4 or 8 bytes total**.

If you have many flags -> **use bit fields or bitsets**.

```cpp
struct Flags {
    unsigned a : 1;
    unsigned b : 1;
    unsigned c : 1;
    unsigned d : 1;
};
```

âœ… All four flags now fit into **1 byte**.

ğŸ§± *Example 5 â€” Real memory difference*

```cpp
struct Misaligned {
    char a;
    int b;
    char c;
};
```
>
- Likely size: **12 bytes** (padding before and after `int`)

```cpp
struct Aligned {
    int b;
    char a;
    char c;
};
```
>
- Likely size: 8 bytes
> 
âœ… 33% smaller!

ğŸ“¦ *Why It Matters*

| Area | Benefit | 
| ----- | ----- |
| ğŸ§  **Cache efficiency** | Smaller structs = more fit into one cache line (faster access) |
| ğŸ’¾ **Memory footprint** | Less padding = smaller memory usage, better for large arrays |
| ğŸš€ **Performance** | Reduced cache misses, faster iteration |
| âš™ï¸ **Portability** | Understanding alignment helps write portable code across platforms |

ğŸ§­ *Quick Optimization Checklist*

| Step | What to Do                                                           |
| ---- | -------------------------------------------------------------------- |
| 1ï¸âƒ£  | Sort fields by **decreasing alignment** (largest â†’ smallest).         |
| 2ï¸âƒ£  | Group same-sized fields together.                                     |
| 3ï¸âƒ£  | Use **bit fields** or `std::bitset` for many bools/flags.             |
| 4ï¸âƒ£  | Align manually only if needed (e.g., `alignas(8)` or `#pragma pack`). |
| 5ï¸âƒ£  | Measure with `sizeof()` â€” never assume layout across compilers!       |

>
âš ï¸ *Note on `#pragma pack`*
>
You can use `#pragma pack(1)` to force tighter packing:
>
```cpp
#pragma pack(push, 1)
struct Packed {
    char a;
    double b;
    int c;
};
#pragma pack(pop)
```
>
This removes all padding â€” but beware:
>
- It may cause **misaligned memory accesses**, which can **slow down** or even **crash** on some architectures.
- Use only when you truly need tight layout (e.g. reading binary files, network packets).

---

# âš™ï¸ Compiler Optimizations: Loop Unrolling & Inlining

ğŸ” **1. Loop Unrolling**

ğŸ§  *What It Is*

**Loop unrolling** is an optimization where the compiler (or programmer) **expands the loop body** multiple times to reduce:
- The number of loop control operations (increment, compare, branch)
- The overhead of looping instructions
- Branch mispredictionse

Essentially:
>Replace a loop that does small work many times with fewer iterations that do more work each time.

ğŸ” *Example â€” Before (normal loop)*
```cpp
for (int i = 0; i < 4; ++i)
    sum += arr[i];
```
âœ… *After (unrolled version)*
```cpp
sum += arr[0];
sum += arr[1];
sum += arr[2];
sum += arr[3];
```
Here, the loop overhead (incrementing `i`, checking condition, branching) is **completely eliminated.**

ğŸ§© *Partial Unrolling Example*

```cpp
// Original
for (int i = 0; i < N; ++i)
    sum += arr[i];

// Unrolled 4Ã—
for (int i = 0; i < N; i += 4) {
    sum += arr[i];
    sum += arr[i + 1];
    sum += arr[i + 2];
    sum += arr[i + 3];
}
```
>
âœ… Fewer loop iterations
>
âœ… Better instruction-level parallelism
>
âœ… Can let the CPU pipeline work more efficiently

ğŸš€ *Benefits*

| Benefit | Why It Helps |
| ----- | ----- |
| â±ï¸ Less loop overhead | Fewer increments & condition checks |
| ğŸ§® Better instruction scheduling | CPU can execute independent operations in parallel |
| ğŸ§  Better cache and branch prediction | Fewer jumps â†’ fewer pipeline stalls |
| âš™ï¸ SIMD vectorization friendly | Easier for compiler to auto-vectorize |

âš ï¸ *Downsides*

| Issue | Explanation |
| ----- | ----- |
| ğŸ’¾ Larger code size | Expanding the loop increases binary size (bad for instruction cache) |
| âŒ Diminishing returns | After a point, unrolling doesnâ€™t improve speed further |
| ğŸ¤– Sometimes compiler does it already | Modern compilers auto-unroll small loops with optimization flags (`-O2`, `-O3`) |

âš™ï¸ *Compiler Example*

You can manually tell the compiler:

```cpp
#pragma unroll 4
for (int i = 0; i < N; ++i)
    sum += arr[i];
```

or rely on optimization flags:
- `O2` (moderate optimization)
- `O3` (aggressive unrolling + inlining + vectorization)

ğŸ§© **2. Function Inlining**

ğŸ§  *What It Is*
**Inlining** replaces a **function call** with the **functionâ€™s actual body**.
Instead of jumping to another memory address, executing code, and returning, the compiler **copies** the function code directly where itâ€™s called.

>
Removes function call overhead â€” faster at runtime, but increases code size.

ğŸ” *Example â€” Before (normal function call)*

```cpp
int square(int x) {
    return x * x;
}

int main() {
    int y = square(5);
}
```
At runtime:
- CPU must **push arguments** on stack/registers
- **Jump** to `square()`
- **Execute** it
- **Return** to caller

âœ… *After (inlined version)*

```cpp
int main() {
    int y = 5 * 5; // inlined
}
```
No function call, no jump â€” just direct computation.

ğŸš€ *Benefits*

| Benefit | Explanation |
| ----- | ----- |
| â±ï¸ Eliminates call overhead | No stack frame setup or return jump |
| âš™ï¸ Enables further optimizations | Compiler can now optimize across function boundaries |
| ğŸ” Often pairs with loop unrolling | Short inlined functions can be unrolled easily |
| ğŸ§  Helps constant folding | Arguments known at compile time â†’ compiler can precompute results |

âš ï¸ *Downsides*

| Issue | Explanation |
| ----- | ----- |
| ğŸ’¾ Larger code size | Every call site duplicates the function body |
| ğŸ§© Instruction cache pressure | More code â†’ less efficient caching |
| âŒ Inlining large or recursive functions | Can explode code size and slow down overall |
| ğŸ¤– Compiler may ignore `inline` | The `inline` keyword is a *suggestion*, not a command |

>
ğŸ§­ *When to Use*
>
âœ… Tiny, performance-critical functions (e.g. getters, math ops, utility inline templates)
>
âœ… Called in inner loops or very frequently
>
âŒ Avoid inlining large functions â€” code bloat can slow you down

âš™ï¸ *How to Inline in C++*

```cpp
inline int square(int x) {
    return x * x;
}
```

or inside classes (implicitly inline):

```cpp
class Math {
public:
    int add(int a, int b) { return a + b; } // automatically inline candidate
};
```

ğŸ§® *Example â€” Inlining in Loops*

```cpp
inline int addOne(int x) { return x + 1; }

int main() {
    for (int i = 0; i < 1'000'000; ++i)
        arr[i] = addOne(arr[i]);
}
```
>
â¡ï¸ The compiler replaces the call with `arr[i] = arr[i] + 1;` directly inside the loop.
>
âœ… No function call overhead per iteration
>
âœ… Faster execution

ğŸ§­ *Combined Example: Unrolling + Inlining*

```cpp
inline int addOne(int x) { return x + 1; }

void process(int *arr, int N) {
    for (int i = 0; i < N; i += 4) {
        arr[i]   = addOne(arr[i]);
        arr[i+1] = addOne(arr[i+1]);
        arr[i+2] = addOne(arr[i+2]);
        arr[i+3] = addOne(arr[i+3]);
    }
}
```
>
- `addOne()` is inlined â†’ no function call cost.
- **Loop unrolled 4Ã—** â†’ fewer branches and better CPU pipeline utilization.

âœ… Result: very fast inner loop.

ğŸ’¾ Slightly larger code, but much better runtime speed.

---

# âš™ï¸ CPU Prefetching: Hardware vs. Software

ğŸ§  *Why Prefetching Exists*

- CPUs are much faster than main memory (RAM).
- Fetching data from RAM can take hundreds of CPU cycles.
- To avoid waiting, CPUs use caches (L1, L2, L3).
- But caches only help if the needed data is already there.

ğŸ‘‰ **Prefetching** = bringing data into cache before itâ€™s needed.

ğŸ”¹ **1. Hardware Prefetching**

ğŸ§© *What It Is*
>
Hardware prefetching is an **automatic mechanism** where the CPU detects predictable memory access patterns and **loads future data** into cache before itâ€™s used.

The CPU itself decides **what to prefetch, when**, and **how far ahead**.

ğŸ” **How It Works**

- The CPU monitors **recent memory access patterns.**
- If it detects a **regular stride pattern** (like sequential or fixed-step accesses), it automatically prefetches the next cache lines.

Example:
```cpp
for (int i = 0; i < N; ++i)
    sum += arr[i];
```
- Hardware sees: `arr[0], arr[1], arr[2], arr[3], â€¦`
- Predicts future accesses â†’ preloads `larr[i+4]`, `arr[i+8]`, etc. into L1/L2 cache.

By the time the loop reaches those elements â€” theyâ€™re already cached âœ…

ğŸš€ *Benefits*

| Benefit | Why It Helps |
| ----- | ----- |
| âœ… Automatic | No programmer effort |
| âš¡ Hides memory latency | Data ready before use |
| ğŸ§  Works well for linear access | Prefetcher detects sequential patterns easily |

âš ï¸ *Limitations*

| Issue | Description |
| -----  | ----- |
| âŒ Unpredictable patterns | Random access (e.g., linked lists) defeats hardware prefetch |
| ğŸ’¾ Limited lookahead | Only prefetches a few cache lines ahead |
| âš™ï¸ Can waste bandwidth | May prefetch unneeded data (cache pollution) |
| ğŸš« Cannot handle dependent memory loads | e.g. pointer chasing: `p = p->next;` |

ğŸ§© *Example of a pattern it canâ€™t optimize:*
```cpp
Node* p = head;
while (p) {
    sum += p->value;
    p = p->next;  // unpredictable address â†’ canâ€™t prefetch
}
```
The next nodeâ€™s address is unknown until `p` is read â€” hardware canâ€™t guess future addresses here.

ğŸ”¹ **2. Software Prefetching**

ğŸ§© *What It Is*
>
Software prefetching means **the programmer explicitly tells the CPU** to load specific memory locations into cache before theyâ€™re used.

You give hints to the CPU using **special intrinsics or instructions**, like:
```cpp
__builtin_prefetch(&arr[i + 64]);
```
These donâ€™t load data into registers â€” they just **initiate a cache load** in the background.

ğŸ” *How It Works*

- You estimate how many iterations ahead the CPU should prefetch.
- Typically 100â€“400 cycles (a few cache lines) before the data is needed.
- The prefetch runs asynchronously â€” the CPU doesnâ€™t wait for it.

Example:
```cpp
for (int i = 0; i < N; ++i) {
    __builtin_prefetch(&arr[i + 16]);  // prefetch future element
    sum += arr[i];
}
```
Now, when iteration `i + 16` happens, the data is likely already in cache âœ…

ğŸš€ *Benefits*

| Benefit | Why It Helps |
| ----- | ----- |
| ğŸ§  Works for irregular access | Programmer knows access pattern even if hardware canâ€™t detect it |
| âš™ï¸ Fine-grained control | You can choose what and how far to prefetch |
| ğŸš… Reduces cache misses | Especially for pointer-heavy or irregular workloads |

âš ï¸ *Drawbacks*

| Issue | Description |
| ----- | ----- |
| ğŸ§© Programmer complexity  | Must be placed carefully â€” too early = eviction, too late = useless |
| ğŸ’¾ Architecture-dependent | Prefetch distance depends on cache/memory latency |
| ğŸš« Not always faster | Compiler/hardware may already optimize enough |
| âš ï¸ Manual tuning needed | Needs profiling and experimentation |

ğŸ”§ *Example with Pointers*

```cpp
for (Node* p = head; p; p = p->next) {
    if (p->next)
        __builtin_prefetch(p->next);  // prefetch next node
    sum += p->value;
}
```
This helps pointer-chasing loops, where hardware prefetchers struggle.

ğŸ” *Hardware vs. Software Prefetch â€” Comparison*

| Feature | Hardware Prefetch | Software Prefetch |
| ----- | ----- | ----- |
| **Who controls it** | CPU (automatic) | Programmer (manual hint) |
| **When it works best** | Sequential or strided access | Irregular or pointer-based access |
| **Tuning needed** | None | Yes â€” depends on latency and pattern |
| **Overhead** | None visible | Slight (extra instruction) |
| **Risk** | Wrong guesses waste cache space | Poor placement can hurt performance |
| **Availability** | Always on | Requires intrinsics (e.g., `__builtin_prefetch`) |

---

# âš™ï¸ Speculative Prefetching & Helper Threads